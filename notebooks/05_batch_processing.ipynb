{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# âš¡ Batch Processing - Scale to 1000s of URLs\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vzucher/brightdata-sdk-python/blob/master/notebooks/05_batch_processing.ipynb)\n",
        "\n",
        "Learn how to efficiently scrape thousands of URLs with progress tracking, error handling, and cost management.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Progress bars with tqdm\n",
        "2. Error handling at scale\n",
        "3. Cost tracking and budgets\n",
        "4. Caching for development\n",
        "5. Parallel processing\n",
        "6. Resume interrupted jobs\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install brightdata-sdk pandas tqdm joblib -q\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import joblib\n",
        "from brightdata import BrightDataClient\n",
        "\n",
        "API_TOKEN = \"your_api_token_here\"\n",
        "client = BrightDataClient(token=API_TOKEN)\n",
        "print(\"âœ… Ready for batch processing!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š 1. Progress Bars with tqdm\n",
        "\n",
        "Always show progress when scraping multiple URLs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample URLs\n",
        "urls = [\n",
        "    f\"https://www.amazon.com/dp/B0{i:08d}\" \n",
        "    for i in range(10)  # Start with 10 for demo\n",
        "]\n",
        "\n",
        "results = []\n",
        "total_cost = 0\n",
        "\n",
        "# Scrape with progress bar\n",
        "for url in tqdm(urls, desc=\"Scraping Amazon products\"):\n",
        "    try:\n",
        "        result = client.scrape.amazon.products(url=url)\n",
        "        \n",
        "        if result.success:\n",
        "            results.append({\n",
        "                'url': url,\n",
        "                'title': result.data.get('title', 'N/A'),\n",
        "                'price': result.data.get('final_price', 'N/A'),\n",
        "                'cost': result.cost,\n",
        "                'status': 'success'\n",
        "            })\n",
        "            total_cost += result.cost\n",
        "        else:\n",
        "            results.append({\n",
        "                'url': url,\n",
        "                'error': result.error,\n",
        "                'cost': 0,\n",
        "                'status': 'failed'\n",
        "            })\n",
        "    except Exception as e:\n",
        "        results.append({\n",
        "            'url': url,\n",
        "            'error': str(e),\n",
        "            'cost': 0,\n",
        "            'status': 'error'\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(f\"\\nâœ… Processed {len(df)} URLs\")\n",
        "print(f\"ðŸ’° Total cost: ${total_cost:.4f}\")\n",
        "print(f\"âœ… Success: {(df['status'] == 'success').sum()}\")\n",
        "print(f\"âŒ Failed: {(df['status'] != 'success').sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’° 2. Cost Management and Budgets\n",
        "\n",
        "Stop scraping when you reach a budget limit:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set a budget\n",
        "BUDGET_LIMIT = 1.00  # $1.00 budget\n",
        "total_cost = 0\n",
        "results_with_budget = []\n",
        "\n",
        "print(f\"ðŸ’° Budget: ${BUDGET_LIMIT:.2f}\")\n",
        "\n",
        "for url in tqdm(urls, desc=\"Scraping with budget\"):\n",
        "    # Check budget\n",
        "    if total_cost >= BUDGET_LIMIT:\n",
        "        print(f\"\\nâš ï¸  Budget limit reached! Stopping at ${total_cost:.4f}\")\n",
        "        break\n",
        "    \n",
        "    try:\n",
        "        result = client.scrape.amazon.products(url=url)\n",
        "        total_cost += result.cost\n",
        "        \n",
        "        if result.success:\n",
        "            results_with_budget.append({\n",
        "                'url': url,\n",
        "                'cost': result.cost,\n",
        "                'cumulative_cost': total_cost\n",
        "            })\n",
        "            \n",
        "        # Warn when approaching limit\n",
        "        if total_cost > BUDGET_LIMIT * 0.8:\n",
        "            print(f\"\\nâš ï¸  80% of budget used: ${total_cost:.4f}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nâœ… Scraped {len(results_with_budget)} URLs\")\n",
        "print(f\"ðŸ’° Final cost: ${total_cost:.4f}\")\n",
        "print(f\"ðŸ“Š Budget used: {100 * total_cost / BUDGET_LIMIT:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup cache\n",
        "memory = joblib.Memory('.cache', verbose=0)\n",
        "\n",
        "@memory.cache\n",
        "def scrape_cached(url):\n",
        "    \"\"\"Cached scraping - only scrapes once per URL.\"\"\"\n",
        "    result = client.scrape.amazon.products(url=url)\n",
        "    return result.to_dict()\n",
        "\n",
        "# First run - hits API\n",
        "print(\"First run (hits API):\")\n",
        "result1 = scrape_cached(urls[0])\n",
        "print(f\"âœ… Scraped: {urls[0][:50]}\")\n",
        "\n",
        "# Second run - uses cache (free!)\n",
        "print(\"\\nSecond run (uses cache):\")\n",
        "result2 = scrape_cached(urls[0])\n",
        "print(f\"âœ… From cache: {urls[0][:50]}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Tip: Delete .cache folder to refresh cached data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”„ 4. Resume Interrupted Jobs\n",
        "\n",
        "Save progress and resume if interrupted:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CHECKPOINT_FILE = 'scraping_progress.csv'\n",
        "\n",
        "# Load previous progress if exists\n",
        "if os.path.exists(CHECKPOINT_FILE):\n",
        "    progress_df = pd.read_csv(CHECKPOINT_FILE)\n",
        "    completed_urls = set(progress_df['url'].tolist())\n",
        "    print(f\"ðŸ“‚ Resuming: {len(completed_urls)} URLs already completed\")\n",
        "else:\n",
        "    progress_df = pd.DataFrame()\n",
        "    completed_urls = set()\n",
        "    print(\"ðŸ†• Starting fresh\")\n",
        "\n",
        "# Process remaining URLs\n",
        "remaining_urls = [url for url in urls if url not in completed_urls]\n",
        "print(f\"ðŸ“‹ {len(remaining_urls)} URLs to process\")\n",
        "\n",
        "for url in tqdm(remaining_urls, desc=\"Scraping\"):\n",
        "    try:\n",
        "        result = client.scrape.amazon.products(url=url)\n",
        "        \n",
        "        # Save progress after each successful scrape\n",
        "        if result.success:\n",
        "            new_row = pd.DataFrame([{\n",
        "                'url': url,\n",
        "                'title': result.data.get('title'),\n",
        "                'cost': result.cost,\n",
        "                'timestamp': pd.Timestamp.now()\n",
        "            }])\n",
        "            progress_df = pd.concat([progress_df, new_row], ignore_index=True)\n",
        "            progress_df.to_csv(CHECKPOINT_FILE, index=False)\n",
        "            \n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"\\nâš ï¸  Interrupted! Progress saved to {CHECKPOINT_FILE}\")\n",
        "        print(f\"âœ… Completed: {len(progress_df)} URLs\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error on {url}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nâœ… Total completed: {len(progress_df)} URLs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š 5. Batch Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze batch results\n",
        "if len(df) > 0:\n",
        "    print(\"ðŸ“Š Batch Processing Summary:\")\n",
        "    print(f\"  Total URLs: {len(df)}\")\n",
        "    print(f\"  Success rate: {100 * (df['status'] == 'success').sum() / len(df):.1f}%\")\n",
        "    print(f\"  Total cost: ${df['cost'].sum():.4f}\")\n",
        "    print(f\"  Avg cost per URL: ${df['cost'].mean():.4f}\")\n",
        "    print(f\"  Avg cost per success: ${df[df['status'] == 'success']['cost'].mean():.4f}\")\n",
        "    \n",
        "    # Export final results\n",
        "    df.to_csv('batch_results_final.csv', index=False)\n",
        "    print(f\"\\nâœ… Exported to batch_results_final.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’¡ Pro Tips for Large-Scale Scraping\n",
        "\n",
        "### 1. Batch Size Optimization\n",
        "```python\n",
        "# Process in batches of 100\n",
        "batch_size = 100\n",
        "for i in range(0, len(urls), batch_size):\n",
        "    batch = urls[i:i+batch_size]\n",
        "    # Process batch\n",
        "```\n",
        "\n",
        "### 2. Rate Limiting (Built-in!)\n",
        "The SDK automatically handles rate limiting - no need to add delays!\n",
        "\n",
        "### 3. Error Recovery\n",
        "```python\n",
        "max_retries = 3\n",
        "for retry in range(max_retries):\n",
        "    try:\n",
        "        result = client.scrape.amazon.products(url=url)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        if retry == max_retries - 1:\n",
        "            print(f\"Failed after {max_retries} retries\")\n",
        "```\n",
        "\n",
        "### 4. Memory Management\n",
        "```python\n",
        "# For very large batches, write to CSV incrementally\n",
        "with open('results.csv', 'a') as f:\n",
        "    for url in urls:\n",
        "        result = scrape(url)\n",
        "        result_df = pd.DataFrame([result])\n",
        "        result_df.to_csv(f, header=f.tell()==0, index=False)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Summary\n",
        "\n",
        "You learned:\n",
        "- âœ… Progress tracking with tqdm\n",
        "- âœ… Budget management and cost tracking\n",
        "- âœ… Caching for development\n",
        "- âœ… Resuming interrupted jobs\n",
        "- âœ… Large-scale scraping best practices\n",
        "\n",
        "## ðŸŽ‰ Congratulations!\n",
        "\n",
        "You've completed all notebooks! You now know how to:\n",
        "1. âœ… Get started quickly\n",
        "2. âœ… Work with pandas DataFrames\n",
        "3. âœ… Scrape Amazon products\n",
        "4. âœ… Analyze LinkedIn jobs\n",
        "5. âœ… Scale to thousands of URLs\n",
        "\n",
        "## ðŸ“š Next Steps\n",
        "\n",
        "- [SDK Documentation](https://github.com/vzucher/brightdata-sdk-python)\n",
        "- [API Reference](https://github.com/vzucher/brightdata-sdk-python/tree/master/docs)\n",
        "- [More Examples](https://github.com/vzucher/brightdata-sdk-python/tree/master/examples)\n",
        "\n",
        "**Happy Large-Scale Scraping! âš¡**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
