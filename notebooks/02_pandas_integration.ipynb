{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üêº Pandas Integration - Data Analysis with Bright Data SDK\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vzucher/brightdata-sdk-python/blob/master/notebooks/02_pandas_integration.ipynb)\n",
        "\n",
        "Learn how to integrate Bright Data SDK with pandas for powerful data analysis.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Converting results to DataFrames\n",
        "2. Batch scraping to DataFrame\n",
        "3. Data cleaning and analysis\n",
        "4. Exporting to CSV/Excel\n",
        "5. Visualization with matplotlib\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install brightdata-sdk pandas matplotlib seaborn -q\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from brightdata import BrightDataClient\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úÖ All packages loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authentication\n",
        "API_TOKEN = \"your_api_token_here\"  # Replace with your token\n",
        "client = BrightDataClient(token=API_TOKEN)\n",
        "print(\"‚úÖ Client initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Method 1: Single Result to DataFrame\n",
        "\n",
        "Convert a single scrape result to a DataFrame:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scrape one product\n",
        "result = client.scrape.amazon.products(\n",
        "    url=\"https://www.amazon.com/dp/B0CRMZHDG8\"\n",
        ")\n",
        "\n",
        "# Convert to DataFrame\n",
        "if result.success and result.data:\n",
        "    df = pd.DataFrame([result.data])\n",
        "    \n",
        "    # Add metadata\n",
        "    df['url'] = result.url\n",
        "    df['cost'] = result.cost\n",
        "    df['elapsed_ms'] = result.elapsed_ms()\n",
        "    df['scraped_at'] = pd.Timestamp.now()\n",
        "    \n",
        "    print(f\"‚úÖ DataFrame: {len(df)} rows, {len(df.columns)} columns\")\n",
        "    display(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Method 2: Batch Scraping to DataFrame\n",
        "\n",
        "Scrape multiple URLs and create a comprehensive DataFrame:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of Amazon product URLs\n",
        "urls = [\n",
        "    \"https://www.amazon.com/dp/B0CRMZHDG8\",\n",
        "    \"https://www.amazon.com/dp/B09B9C8K3T\",\n",
        "    \"https://www.amazon.com/dp/B0CX23V2ZK\",\n",
        "]\n",
        "\n",
        "print(f\"Scraping {len(urls)} products...\")\n",
        "results = []\n",
        "\n",
        "for i, url in enumerate(urls, 1):\n",
        "    print(f\"  [{i}/{len(urls)}] {url[:50]}...\")\n",
        "    try:\n",
        "        result = client.scrape.amazon.products(url=url)\n",
        "        if result.success:\n",
        "            results.append({\n",
        "                'url': result.url,\n",
        "                'title': result.data.get('title', 'N/A'),\n",
        "                'price': result.data.get('final_price', 'N/A'),\n",
        "                'rating': result.data.get('rating', 'N/A'),\n",
        "                'reviews_count': result.data.get('reviews_count', 0),\n",
        "                'cost': result.cost,\n",
        "                'elapsed_ms': result.elapsed_ms(),\n",
        "                'status': 'success'\n",
        "            })\n",
        "    except Exception as e:\n",
        "        results.append({'url': url, 'error': str(e), 'status': 'failed'})\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "print(f\"\\n‚úÖ Scraped {len(df)} products\")\n",
        "print(f\"   Success: {(df['status'] == 'success').sum()}\")\n",
        "print(f\"   Failed: {(df['status'] != 'success').sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df.head())\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nüìä Summary:\")\n",
        "print(f\"Total cost: ${df['cost'].sum():.4f}\")\n",
        "print(f\"Avg time: {df['elapsed_ms'].mean():.2f}ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Export Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to CSV\n",
        "df.to_csv('amazon_products.csv', index=False)\n",
        "print(\"‚úÖ Exported to amazon_products.csv\")\n",
        "\n",
        "# Export to Excel\n",
        "df.to_excel('amazon_products.xlsx', index=False, sheet_name='Products')\n",
        "print(\"‚úÖ Exported to amazon_products.xlsx\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Pro Tips for Data Scientists\n",
        "\n",
        "### Use Progress Bars\n",
        "```python\n",
        "from tqdm import tqdm\n",
        "for url in tqdm(urls, desc=\"Scraping\"):\n",
        "    result = client.scrape.amazon.products(url=url)\n",
        "```\n",
        "\n",
        "### Cache Results\n",
        "```python\n",
        "import joblib\n",
        "memory = joblib.Memory('.cache', verbose=0)\n",
        "\n",
        "@memory.cache\n",
        "def scrape_cached(url):\n",
        "    return client.scrape.amazon.products(url=url)\n",
        "```\n",
        "\n",
        "### Track Costs\n",
        "```python\n",
        "total_cost = df['cost'].sum()\n",
        "print(f\"Total spent: ${total_cost:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "You learned:\n",
        "- ‚úÖ Converting SDK results to DataFrames\n",
        "- ‚úÖ Batch scraping workflows\n",
        "- ‚úÖ Data visualization\n",
        "- ‚úÖ Exporting to CSV/Excel\n",
        "\n",
        "## üéì Next: [Amazon Deep Dive](./03_amazon_scraping.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
