"""Web Unlocker API - High-level service wrapper for Bright Data's Web Unlocker proxy service."""

from typing import Union, List, Optional, Dict, Any
from datetime import datetime, timezone
import asyncio

from .base import BaseAPI
from ..models import ScrapeResult
from ..utils.validation import (
    validate_url,
    validate_url_list,
    validate_zone_name,
    validate_country_code,
    validate_timeout,
    validate_response_format,
    validate_http_method,
)
from ..utils.url import extract_root_domain
from ..exceptions import ValidationError, APIError


class WebUnlockerService(BaseAPI):
    """
    High-level service wrapper around Bright Data's Web Unlocker proxy service.
    
    Provides simple HTTP-based scraping with anti-bot capabilities. This is the
    fastest, most cost-effective option for basic HTML extraction without JavaScript rendering.
    
    Example:
        >>> async with AsyncEngine(token) as engine:
        ...     service = WebUnlockerService(engine)
        ...     result = await service.scrape_async("https://example.com", zone="my_zone")
        ...     print(result.data)
    """
    
    ENDPOINT = "/request"
    
    async def _execute_async(self, *args: Any, **kwargs: Any) -> Any:
        """Execute API operation asynchronously."""
        return await self.scrape_async(*args, **kwargs)
    
    async def scrape_async(
        self,
        url: Union[str, List[str]],
        zone: str,
        country: str = "",
        response_format: str = "raw",
        method: str = "GET",
        timeout: Optional[int] = None,
    ) -> Union[ScrapeResult, List[ScrapeResult]]:
        """
        Scrape URL(s) asynchronously using Web Unlocker API.
        
        Args:
            url: Single URL string or list of URLs to scrape.
            zone: Bright Data zone identifier.
            country: Two-letter ISO country code for proxy location (optional).
            response_format: Response format - "json" for structured data, "raw" for HTML string.
            method: HTTP method for the request (default: "GET").
            timeout: Request timeout in seconds (uses engine default if not provided).
        
        Returns:
            ScrapeResult for single URL, or List[ScrapeResult] for multiple URLs.
        
        Raises:
            ValidationError: If input validation fails.
            APIError: If API request fails.
        """
        validate_zone_name(zone)
        validate_response_format(response_format)
        validate_http_method(method)
        validate_country_code(country)
        
        if timeout is not None:
            validate_timeout(timeout)
        
        if isinstance(url, list):
            validate_url_list(url)
            return await self._scrape_multiple_async(
                urls=url,
                zone=zone,
                country=country,
                response_format=response_format,
                method=method,
                timeout=timeout,
            )
        else:
            validate_url(url)
            return await self._scrape_single_async(
                url=url,
                zone=zone,
                country=country,
                response_format=response_format,
                method=method,
                timeout=timeout,
            )
    
    async def _scrape_single_async(
        self,
        url: str,
        zone: str,
        country: str,
        response_format: str,
        method: str,
        timeout: Optional[int],
    ) -> ScrapeResult:
        """Scrape a single URL."""
        request_sent_at = datetime.now(timezone.utc)
        
        payload: Dict[str, Any] = {
            "zone": zone,
            "url": url,
            "format": response_format,
            "method": method,
        }
        
        if country:
            payload["country"] = country.upper()
        
        try:
            # Make the request and read response body immediately
            async with self.engine._session.post(
                f"{self.engine.BASE_URL}{self.ENDPOINT}",
                json=payload,
                headers=self.engine._session.headers
            ) as response:
                data_received_at = datetime.now(timezone.utc)
                
                if response.status == 200:
                    if response_format == "json":
                        try:
                            data = await response.json()
                        except Exception as e:
                            raise APIError(f"Failed to parse JSON response: {str(e)}")
                    else:
                        data = await response.text()
                    
                    root_domain = extract_root_domain(url)
                    html_char_size = len(data) if isinstance(data, str) else None
                    
                    return ScrapeResult(
                        success=True,
                        url=url,
                        status="ready",
                        data=data,
                        cost=None,
                        request_sent_at=request_sent_at,
                        data_received_at=data_received_at,
                        root_domain=root_domain,
                        html_char_size=html_char_size,
                    )
                else:
                    error_text = await response.text()
                    return ScrapeResult(
                        success=False,
                        url=url,
                        status="error",
                        error=f"API returned status {response.status}: {error_text}",
                        request_sent_at=request_sent_at,
                        data_received_at=data_received_at,
                    )
        
        except Exception as e:
            data_received_at = datetime.now(timezone.utc)
            
            if isinstance(e, (ValidationError, APIError)):
                raise
            
            return ScrapeResult(
                success=False,
                url=url,
                status="error",
                error=f"Unexpected error: {str(e)}",
                request_sent_at=request_sent_at,
                data_received_at=data_received_at,
            )
    
    async def _scrape_multiple_async(
        self,
        urls: List[str],
        zone: str,
        country: str,
        response_format: str,
        method: str,
        timeout: Optional[int],
    ) -> List[ScrapeResult]:
        """Scrape multiple URLs concurrently."""
        tasks = [
            self._scrape_single_async(
                url=url,
                zone=zone,
                country=country,
                response_format=response_format,
                method=method,
                timeout=timeout,
            )
            for url in urls
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        processed_results: List[ScrapeResult] = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(
                    ScrapeResult(
                        success=False,
                        url=urls[i],
                        status="error",
                        error=f"Exception: {str(result)}",
                        request_sent_at=datetime.now(timezone.utc),
                        data_received_at=datetime.now(timezone.utc),
                    )
                )
            else:
                processed_results.append(result)
        
        return processed_results
    
    def scrape(
        self,
        url: Union[str, List[str]],
        zone: str,
        country: str = "",
        response_format: str = "raw",
        method: str = "GET",
        timeout: Optional[int] = None,
    ) -> Union[ScrapeResult, List[ScrapeResult]]:
        """
        Scrape URL(s) synchronously.
        
        Args:
            url: Single URL string or list of URLs to scrape.
            zone: Bright Data zone identifier.
            country: Two-letter ISO country code for proxy location (optional).
            response_format: Response format - "json" for structured data, "raw" for HTML string.
            method: HTTP method for the request (default: "GET").
            timeout: Request timeout in seconds.
        
        Returns:
            ScrapeResult for single URL, or List[ScrapeResult] for multiple URLs.
        """
        return self._execute_sync(
            url=url,
            zone=zone,
            country=country,
            response_format=response_format,
            method=method,
            timeout=timeout,
        )
